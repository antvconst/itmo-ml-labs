{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_line(s):\n",
    "    \"\"\"\n",
    "    Deletes everything except latin symbols, whitespaces and sentence endings.\n",
    "    Sentence endings are converted to full stops. All characters are converted\n",
    "    to lower case.\n",
    "    \"\"\"\n",
    "    s = s.replace('-', ' ') \\\n",
    "         .replace('?', '.') \\\n",
    "         .replace('!', '.')\n",
    "    return re.sub(r'[^a-zA-Z \\.]', '', s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lines\n",
    "with open('alice_in_wonderland.txt') as f:\n",
    "    lines = f.readlines()\n",
    "# remove all characters except latin symbols, whitespaces and full stops\n",
    "lines = [sanitize_line(s) for s in lines]\n",
    "# drop empty lines and lines with only whitespaces\n",
    "lines = [s for s in lines if s and set(s) != {' '}]\n",
    "# strip the leading and trailing whitespaces\n",
    "lines = [s.strip() for s in lines]\n",
    "# remove the preamble and the postamble\n",
    "lines = lines[32:-307]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the chapter heading indices\n",
    "chapter_num_indices = [i for i, line in enumerate(lines) if line.startswith('chapter')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the chapters and sentences\n",
    "chapters = []\n",
    "chapter_num_indices.append(len(lines))\n",
    "\n",
    "for i in range(len(chapter_num_indices) - 1):\n",
    "    idx_start = chapter_num_indices[i] + 2\n",
    "    idx_end = chapter_num_indices[i+1]\n",
    "    chapter_lines = lines[idx_start:idx_end]\n",
    "    chapter = ' '.join(chapter_lines)\n",
    "    chapters.append(chapter)\n",
    "\n",
    "sentences_by_chapter_ = [chapter.split('. ') for chapter in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_by_chapter = []\n",
    "for chapter_ in sentences_by_chapter_:\n",
    "    chapter = []\n",
    "    for s in chapter_:\n",
    "        words = s.split()\n",
    "        # remove stops words\n",
    "        words = [w for w in words if w not in stopwords_list]\n",
    "        # lemmatize the words\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "        # reconstruct the sentences\n",
    "        if words:\n",
    "            s = ' '.join(words)\n",
    "            chapter.append(s)\n",
    "    sentences_by_chapter.append(chapter)\n",
    "sentences_by_chapter = sentences_by_chapter[:-1]\n",
    "chapters = [' '.join(sentences) for sentences in sentences_by_chapter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important words by chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(chapters)\n",
    "inverse_vocab = {v: k for k, v in tfidf.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chapter in chapters:\n",
    "    tfidf_enc = tfidf.transform([chapter]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_enc = tfidf.transform(chapters).todense()\n",
    "# negate to sort in descending order\n",
    "tfidf_enc *= -1\n",
    "argsort = tfidf_enc.argsort(axis=1)\n",
    "topk_indices = np.asarray(argsort[:, :11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_words_by_chapter = []\n",
    "for doc in topk_indices:\n",
    "    # decode the words from indices\n",
    "    words = [inverse_vocab[k] for k in doc]\n",
    "    # remove \"alice\" and get first 10\n",
    "    words = [w for w in words if w != 'alice'][:10]\n",
    "    topk_words_by_chapter.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are the most important words (according to tf-idf features) for each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1: little, bat, way, door, key, rabbit, eat, like, think, either\n",
      "Chapter 2: mouse, little, pool, im, dear, swam, cat, said, foot, cried\n",
      "Chapter 3: mouse, said, dodo, prize, lory, race, dry, thimble, know, course\n",
      "Chapter 4: bill, window, little, rabbit, puppy, bottle, fan, glove, chimney, said\n",
      "Chapter 5: caterpillar, said, pigeon, serpent, im, youth, egg, size, father, little\n",
      "Chapter 6: said, footman, cat, baby, mad, duchess, wow, like, pig, cook\n",
      "Chapter 7: hatter, dormouse, said, hare, march, tea, twinkle, time, draw, treacle\n",
      "Chapter 8: queen, said, king, hedgehog, gardener, rose, soldier, cat, executioner, procession\n",
      "Chapter 9: turtle, mock, said, gryphon, duchess, moral, queen, went, school, day\n",
      "Chapter 10: turtle, mock, gryphon, said, lobster, dance, soup, beautiful, join, whiting\n",
      "Chapter 11: king, hatter, court, said, dormouse, witness, jury, queen, juror, officer\n"
     ]
    }
   ],
   "source": [
    "for i, words in enumerate(topk_words_by_chapter):\n",
    "    print(f\"Chapter {i+1}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed chapter names (some)**:\n",
    "\n",
    "1. Little door and little way\n",
    "7. Tea time\n",
    "8. Queen's gardener and executioner\n",
    "11. Queen's jury\n",
    "\n",
    "Sounds stupid and awfully out-of-context :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used verbs in sentences with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [s for c in sentences_by_chapter for s in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = collections.defaultdict(int)\n",
    "for s in all_sentences:\n",
    "    s = s.split()\n",
    "    if 'alice' in s:\n",
    "        for w, pos_tag in nltk.pos_tag(s):\n",
    "            if pos_tag.startswith('VB'):\n",
    "                verbs[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_verbs = sorted(list(verbs.items()), key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said: 153 times\n",
      "thought: 33 times\n",
      "went: 29 times\n",
      "say: 20 times\n",
      "looked: 20 times\n",
      "began: 19 times\n",
      "got: 17 times\n",
      "think: 16 times\n",
      "know: 16 times\n",
      "go: 15 times\n"
     ]
    }
   ],
   "source": [
    "for word, num in most_used_verbs:\n",
    "    print(f'{word}: {num} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Alice mostly says something, thinks about something, walks somewhere and looks at something. Well, we all do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
